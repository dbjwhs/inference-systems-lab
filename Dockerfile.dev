# Inference Systems Lab - CUDA Development Environment
# Multi-stage Docker container for ML inference development with TensorRT and ONNX Runtime
#
# Usage:
#   docker build -f Dockerfile.dev -t inference-lab:dev .
#   docker run --gpus all -it -v $(pwd):/workspace inference-lab:dev
#
# Features:
#   - NVIDIA CUDA 12.3 with cuDNN support
#   - TensorRT 8.6 for GPU inference optimization
#   - ONNX Runtime 1.16 with GPU providers
#   - Modern C++17 toolchain (GCC 11, CMake 3.24+)
#   - Python 3.10 with ML development libraries
#   - Cap'n Proto for high-performance serialization
#   - Google Test and Benchmark frameworks
#   - Development tools (clang-format, clang-tidy, gdb)

# =============================================================================
# Stage 1: Base CUDA Development Environment
# =============================================================================
FROM nvidia/cuda:12.3-devel-ubuntu22.04 as cuda_base

# Avoid interactive prompts during package installation
ENV DEBIAN_FRONTEND=noninteractive
ENV TZ=UTC

# Install system dependencies and development tools
RUN apt-get update && apt-get install -y \
    # Build essentials
    build-essential \
    cmake \
    ninja-build \
    git \
    wget \
    curl \
    unzip \
    pkg-config \
    # Modern C++ toolchain
    gcc-11 \
    g++-11 \
    clang-14 \
    clang-format-14 \
    clang-tidy-14 \
    # Development and debugging tools
    gdb \
    valgrind \
    strace \
    htop \
    vim \
    nano \
    # Python development
    python3.10 \
    python3.10-dev \
    python3-pip \
    # System libraries
    libssl-dev \
    libcurl4-openssl-dev \
    zlib1g-dev \
    libjpeg-dev \
    libpng-dev \
    # Audio/Video libraries (for multimedia ML models)
    libavcodec-dev \
    libavformat-dev \
    libswscale-dev \
    # Cleanup
    && rm -rf /var/lib/apt/lists/*

# Set GCC 11 as default
RUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 100 \
    && update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-11 100

# Create symbolic links for clang tools
RUN ln -sf /usr/bin/clang-format-14 /usr/local/bin/clang-format \
    && ln -sf /usr/bin/clang-tidy-14 /usr/local/bin/clang-tidy

# =============================================================================
# Stage 2: Python ML Libraries and Tools
# =============================================================================
FROM cuda_base as python_ml

# Upgrade pip and install Python development tools
RUN python3 -m pip install --upgrade pip setuptools wheel

# Install essential Python libraries for ML development
RUN pip3 install \
    # Core scientific computing
    numpy>=1.24.0 \
    scipy>=1.10.0 \
    pandas>=2.0.0 \
    # Machine learning frameworks
    torch>=2.1.0 \
    torchvision>=0.16.0 \
    torchaudio>=2.1.0 \
    # ONNX ecosystem
    onnx>=1.15.0 \
    onnxruntime-gpu>=1.16.0 \
    # Visualization and analysis
    matplotlib>=3.7.0 \
    seaborn>=0.12.0 \
    plotly>=5.15.0 \
    # Development and testing
    pytest>=7.4.0 \
    pytest-cov>=4.1.0 \
    pytest-benchmark>=4.0.0 \
    # Code quality
    black>=23.7.0 \
    isort>=5.12.0 \
    flake8>=6.0.0 \
    # Jupyter for interactive development
    jupyter>=1.0.0 \
    jupyterlab>=4.0.0 \
    # Utilities
    tqdm>=4.65.0 \
    requests>=2.31.0 \
    pyyaml>=6.0

# =============================================================================
# Stage 3: Cap'n Proto and Google Libraries
# =============================================================================
FROM python_ml as capnproto_build

# Build and install Cap'n Proto from source for latest version
WORKDIR /tmp/capnproto
RUN wget https://capnproto.org/capnproto-c++-1.0.1.tar.gz \
    && tar zxf capnproto-c++-1.0.1.tar.gz \
    && cd capnproto-c++-1.0.1 \
    && ./configure --prefix=/usr/local \
    && make -j$(nproc) \
    && make install \
    && ldconfig \
    && rm -rf /tmp/capnproto

# Build and install Google Test and Benchmark from source
WORKDIR /tmp/googletest
RUN git clone --depth 1 --branch v1.14.0 https://github.com/google/googletest.git . \
    && cmake -B build -G Ninja \
        -DCMAKE_BUILD_TYPE=Release \
        -DCMAKE_INSTALL_PREFIX=/usr/local \
        -DBUILD_SHARED_LIBS=ON \
    && cmake --build build -j$(nproc) \
    && cmake --install build \
    && rm -rf /tmp/googletest

WORKDIR /tmp/benchmark
RUN git clone --depth 1 --branch v1.8.3 https://github.com/google/benchmark.git . \
    && cmake -B build -G Ninja \
        -DCMAKE_BUILD_TYPE=Release \
        -DCMAKE_INSTALL_PREFIX=/usr/local \
        -DBENCHMARK_ENABLE_GTEST_TESTS=OFF \
        -DBUILD_SHARED_LIBS=ON \
    && cmake --build build -j$(nproc) \
    && cmake --install build \
    && rm -rf /tmp/benchmark

# Update library cache
RUN ldconfig

# =============================================================================
# Stage 4: TensorRT Installation
# =============================================================================
FROM capnproto_build as tensorrt_install

# Install TensorRT 8.6 from NVIDIA repositories
# Note: Requires NVIDIA Developer account and acceptance of license
RUN apt-get update && apt-get install -y \
    # TensorRT runtime libraries
    libnvinfer8=8.6.1-1+cuda12.0 \
    libnvinfer-plugin8=8.6.1-1+cuda12.0 \
    libnvparsers8=8.6.1-1+cuda12.0 \
    libnvonnxparsers8=8.6.1-1+cuda12.0 \
    # TensorRT development packages
    libnvinfer-dev=8.6.1-1+cuda12.0 \
    libnvinfer-plugin-dev=8.6.1-1+cuda12.0 \
    libnvparsers-dev=8.6.1-1+cuda12.0 \
    libnvonnxparsers-dev=8.6.1-1+cuda12.0 \
    # Python bindings (optional)
    python3-libnvinfer=8.6.1-1+cuda12.0 \
    && rm -rf /var/lib/apt/lists/*

# Install TensorRT Python package
RUN pip3 install tensorrt==8.6.1

# =============================================================================
# Stage 5: Development Environment Setup
# =============================================================================
FROM tensorrt_install as dev_environment

# Set up working directory
WORKDIR /workspace

# Environment variables for development
ENV CUDA_HOME=/usr/local/cuda
ENV PATH="${CUDA_HOME}/bin:${PATH}"
ENV LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}"
ENV CUDNN_VERSION=8
ENV TENSORRT_VERSION=8.6.1

# Configure CMake to find CUDA and TensorRT
ENV CMAKE_CUDA_COMPILER=${CUDA_HOME}/bin/nvcc
ENV TensorRT_ROOT=/usr/local

# Create development user (non-root for security)
RUN useradd -m -s /bin/bash -u 1000 developer \
    && usermod -aG sudo developer \
    && echo "developer ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers

# Create common development directories
RUN mkdir -p /workspace/{build,data,models,experiments} \
    && chown -R developer:developer /workspace

# Install development utilities as developer user
USER developer

# Configure git (will be overridden by user's config when mounted)
RUN git config --global user.name "Developer" \
    && git config --global user.email "developer@inference-lab.local" \
    && git config --global init.defaultBranch main \
    && git config --global core.editor vim

# Create useful aliases and environment setup
RUN echo 'alias ll="ls -la"' >> ~/.bashrc \
    && echo 'alias la="ls -A"' >> ~/.bashrc \
    && echo 'alias l="ls -CF"' >> ~/.bashrc \
    && echo 'alias ..="cd .."' >> ~/.bashrc \
    && echo 'alias cmake-debug="cmake -DCMAKE_BUILD_TYPE=Debug"' >> ~/.bashrc \
    && echo 'alias cmake-release="cmake -DCMAKE_BUILD_TYPE=Release"' >> ~/.bashrc \
    && echo 'alias ninja-build="cmake --build . --parallel"' >> ~/.bashrc \
    && echo 'export PS1="\[\033[01;32m\]\u@inference-lab\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]\$ "' >> ~/.bashrc

# =============================================================================
# Stage 6: Final Development Image
# =============================================================================
FROM dev_environment as final

# Copy project-specific configuration files
COPY --chown=developer:developer .clang-format /workspace/
COPY --chown=developer:developer .clang-tidy /workspace/
COPY --chown=developer:developer .gitignore /workspace/

# Create example configuration for the project
RUN cat > /workspace/cmake_config.sh << 'EOF'
#!/bin/bash
# Example CMake configuration script for Inference Systems Lab

# Debug build with all features
cmake_debug() {
    cmake -B build/debug -G Ninja \
        -DCMAKE_BUILD_TYPE=Debug \
        -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc \
        -DENABLE_TENSORRT=ON \
        -DENABLE_ONNX_RUNTIME=ON \
        -DENABLE_CUDA=ON \
        -DCMAKE_EXPORT_COMPILE_COMMANDS=ON \
        "$@"
}

# Release build optimized for performance
cmake_release() {
    cmake -B build/release -G Ninja \
        -DCMAKE_BUILD_TYPE=Release \
        -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc \
        -DENABLE_TENSORRT=ON \
        -DENABLE_ONNX_RUNTIME=ON \
        -DENABLE_CUDA=ON \
        -DCMAKE_EXPORT_COMPILE_COMMANDS=ON \
        "$@"
}

# Build with sanitizers for development
cmake_sanitizers() {
    cmake -B build/sanitizers -G Ninja \
        -DCMAKE_BUILD_TYPE=Debug \
        -DSANITIZER_TYPE=address+undefined \
        -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc \
        -DENABLE_CUDA=OFF \
        "$@"
}

echo "Available commands:"
echo "  cmake_debug     - Debug build with CUDA/TensorRT support"
echo "  cmake_release   - Optimized release build"
echo "  cmake_sanitizers - Debug build with AddressSanitizer"
EOF

RUN chmod +x /workspace/cmake_config.sh

# Create development startup script
RUN cat > /workspace/dev_setup.sh << 'EOF'
#!/bin/bash
# Development environment setup script

echo "ðŸš€ Inference Systems Lab - Development Environment"
echo "=================================================="
echo ""
echo "Environment Information:"
echo "  CUDA Version: $(nvcc --version | grep -o 'release [0-9.]*' | cut -d' ' -f2)"
echo "  TensorRT: ${TENSORRT_VERSION}"
echo "  Python: $(python3 --version)"
echo "  CMake: $(cmake --version | head -1)"
echo "  GCC: $(gcc --version | head -1)"
echo ""
echo "GPU Information:"
nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader,nounits 2>/dev/null || echo "  No GPU detected or nvidia-smi not available"
echo ""
echo "Quick Start:"
echo "  1. Source cmake helper functions: source cmake_config.sh"
echo "  2. Configure project: cmake_debug"
echo "  3. Build project: cmake --build build/debug"
echo "  4. Run tests: cd build/debug && ctest"
echo ""
echo "Development Tools Available:"
echo "  - clang-format, clang-tidy (code formatting and analysis)"
echo "  - gdb, valgrind (debugging and profiling)"
echo "  - pytest, jupyter (Python development)"
echo "  - CUDA toolkit, TensorRT, ONNX Runtime"
echo ""
EOF

RUN chmod +x /workspace/dev_setup.sh

# Set default working directory and startup command
WORKDIR /workspace

# Health check to verify CUDA and development tools
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD nvcc --version && python3 -c "import torch; print(f'PyTorch CUDA available: {torch.cuda.is_available()}')" || exit 1

# Default command shows development environment info
CMD ["/bin/bash", "-c", "./dev_setup.sh && /bin/bash"]

# =============================================================================
# Build Information and Labels
# =============================================================================
LABEL maintainer="Inference Systems Lab <inference-lab@example.com>"
LABEL description="CUDA development environment for ML inference systems"
LABEL version="1.0"
LABEL cuda.version="12.3"
LABEL tensorrt.version="8.6.1"
LABEL onnxruntime.version="1.16.0"
LABEL build.date="2025-08-19"

# Document exposed ports (for Jupyter, TensorBoard, etc.)
EXPOSE 8888 6006 8080

# Create volumes for common development needs
VOLUME ["/workspace", "/data", "/models"]